{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe the dataset to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_preprocessed: [[[[ 0.2421875  0.2109375  0.21875  ]\n",
      "   [ 0.2421875  0.2109375  0.21875  ]\n",
      "   [ 0.2421875  0.2109375  0.21875  ]\n",
      "   ...\n",
      "   [ 0.2578125  0.1875     0.2109375]\n",
      "   [ 0.234375   0.203125   0.1953125]\n",
      "   [ 0.21875    0.2109375  0.1953125]]\n",
      "\n",
      "  [[ 0.2421875  0.2109375  0.21875  ]\n",
      "   [ 0.2421875  0.2109375  0.21875  ]\n",
      "   [ 0.2421875  0.2109375  0.21875  ]\n",
      "   ...\n",
      "   [ 0.2578125  0.1875     0.2109375]\n",
      "   [ 0.234375   0.203125   0.1953125]\n",
      "   [ 0.21875    0.2109375  0.1953125]]\n",
      "\n",
      "  [[ 0.2421875  0.2109375  0.21875  ]\n",
      "   [ 0.2421875  0.2109375  0.21875  ]\n",
      "   [ 0.25       0.203125   0.21875  ]\n",
      "   ...\n",
      "   [ 0.2578125  0.1875     0.1953125]\n",
      "   [ 0.234375   0.203125   0.1953125]\n",
      "   [ 0.21875    0.2109375  0.1953125]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.625     -0.6171875 -0.5859375]\n",
      "   [-0.65625   -0.6484375 -0.6171875]\n",
      "   [-0.6640625 -0.65625   -0.6171875]\n",
      "   ...\n",
      "   [ 0.4609375  0.4140625  0.4296875]\n",
      "   [ 0.4765625  0.40625    0.4296875]\n",
      "   [ 0.46875    0.3984375  0.40625  ]]\n",
      "\n",
      "  [[-0.625     -0.6328125 -0.59375  ]\n",
      "   [-0.671875  -0.6640625 -0.6328125]\n",
      "   [-0.6484375 -0.640625  -0.6015625]\n",
      "   ...\n",
      "   [ 0.4609375  0.4140625  0.4296875]\n",
      "   [ 0.46875    0.3984375  0.421875 ]\n",
      "   [ 0.4765625  0.3984375  0.40625  ]]\n",
      "\n",
      "  [[-0.6328125 -0.640625  -0.6015625]\n",
      "   [-0.6796875 -0.6875    -0.6484375]\n",
      "   [-0.640625  -0.6328125 -0.59375  ]\n",
      "   ...\n",
      "   [ 0.4609375  0.4140625  0.4296875]\n",
      "   [ 0.4765625  0.390625   0.421875 ]\n",
      "   [ 0.4765625  0.3984375  0.40625  ]]]\n",
      "\n",
      "\n",
      " [[[ 0.5078125  0.140625   0.015625 ]\n",
      "   [ 0.5078125  0.140625   0.015625 ]\n",
      "   [ 0.5078125  0.140625   0.015625 ]\n",
      "   ...\n",
      "   [ 0.5        0.1171875  0.       ]\n",
      "   [ 0.4765625  0.09375   -0.0234375]\n",
      "   [ 0.46875    0.0859375 -0.03125  ]]\n",
      "\n",
      "  [[ 0.5078125  0.140625   0.015625 ]\n",
      "   [ 0.5078125  0.140625   0.015625 ]\n",
      "   [ 0.5078125  0.140625   0.015625 ]\n",
      "   ...\n",
      "   [ 0.5078125  0.125      0.0078125]\n",
      "   [ 0.4921875  0.109375  -0.0078125]\n",
      "   [ 0.484375   0.1015625 -0.015625 ]]\n",
      "\n",
      "  [[ 0.5078125  0.140625   0.015625 ]\n",
      "   [ 0.5078125  0.140625   0.015625 ]\n",
      "   [ 0.5078125  0.140625   0.015625 ]\n",
      "   ...\n",
      "   [ 0.515625   0.1328125  0.015625 ]\n",
      "   [ 0.5078125  0.125      0.0078125]\n",
      "   [ 0.5        0.1171875  0.       ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.5        0.1015625 -0.03125  ]\n",
      "   [ 0.5        0.1015625 -0.03125  ]\n",
      "   [ 0.5        0.1015625 -0.03125  ]\n",
      "   ...\n",
      "   [ 0.515625   0.1328125  0.015625 ]\n",
      "   [ 0.515625   0.1328125  0.015625 ]\n",
      "   [ 0.515625   0.1328125  0.015625 ]]\n",
      "\n",
      "  [[ 0.5        0.1015625 -0.03125  ]\n",
      "   [ 0.5        0.1015625 -0.03125  ]\n",
      "   [ 0.5        0.1015625 -0.03125  ]\n",
      "   ...\n",
      "   [ 0.5234375  0.140625   0.0234375]\n",
      "   [ 0.5234375  0.140625   0.0234375]\n",
      "   [ 0.53125    0.1484375  0.03125  ]]\n",
      "\n",
      "  [[ 0.5078125  0.109375  -0.0234375]\n",
      "   [ 0.5        0.1015625 -0.03125  ]\n",
      "   [ 0.5        0.1015625 -0.03125  ]\n",
      "   ...\n",
      "   [ 0.5234375  0.140625   0.0234375]\n",
      "   [ 0.53125    0.1484375  0.03125  ]\n",
      "   [ 0.5390625  0.15625    0.0390625]]]\n",
      "\n",
      "\n",
      " [[[ 0.3359375  0.34375    0.359375 ]\n",
      "   [ 0.4140625  0.421875   0.4375   ]\n",
      "   [ 0.3984375  0.40625    0.421875 ]\n",
      "   ...\n",
      "   [ 0.1015625  0.1015625  0.0859375]\n",
      "   [ 0.171875   0.171875   0.15625  ]\n",
      "   [ 0.25       0.25       0.234375 ]]\n",
      "\n",
      "  [[ 0.3515625  0.359375   0.375    ]\n",
      "   [ 0.3984375  0.40625    0.421875 ]\n",
      "   [ 0.390625   0.3984375  0.4140625]\n",
      "   ...\n",
      "   [ 0.0625     0.0625     0.046875 ]\n",
      "   [ 0.1015625  0.1015625  0.0859375]\n",
      "   [ 0.1875     0.1875     0.171875 ]]\n",
      "\n",
      "  [[ 0.3671875  0.375      0.390625 ]\n",
      "   [ 0.375      0.3828125  0.3984375]\n",
      "   [ 0.390625   0.3984375  0.4140625]\n",
      "   ...\n",
      "   [ 0.0703125  0.0703125  0.0546875]\n",
      "   [ 0.0546875  0.0546875  0.0390625]\n",
      "   [ 0.140625   0.140625   0.125    ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.2421875  0.2578125  0.21875  ]\n",
      "   [ 0.3125     0.328125   0.2890625]\n",
      "   [ 0.296875   0.3125     0.2734375]\n",
      "   ...\n",
      "   [ 0.3984375  0.3984375  0.3828125]\n",
      "   [ 0.375      0.375      0.359375 ]\n",
      "   [ 0.3359375  0.3359375  0.3203125]]\n",
      "\n",
      "  [[ 0.265625   0.2890625  0.234375 ]\n",
      "   [ 0.3203125  0.34375    0.2890625]\n",
      "   [ 0.265625   0.2890625  0.234375 ]\n",
      "   ...\n",
      "   [ 0.3828125  0.3828125  0.3671875]\n",
      "   [ 0.3828125  0.3828125  0.3671875]\n",
      "   [ 0.34375    0.34375    0.328125 ]]\n",
      "\n",
      "  [[ 0.328125   0.3515625  0.296875 ]\n",
      "   [ 0.3359375  0.359375   0.3046875]\n",
      "   [ 0.1953125  0.21875    0.1484375]\n",
      "   ...\n",
      "   [ 0.328125   0.328125   0.3125   ]\n",
      "   [ 0.390625   0.390625   0.375    ]\n",
      "   [ 0.375      0.375      0.359375 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.3984375 -0.4296875 -0.515625 ]\n",
      "   [-0.40625   -0.4375    -0.5234375]\n",
      "   [-0.40625   -0.4375    -0.5234375]\n",
      "   ...\n",
      "   [-0.2890625 -0.3203125 -0.40625  ]\n",
      "   [-0.2890625 -0.3203125 -0.40625  ]\n",
      "   [-0.296875  -0.328125  -0.4140625]]\n",
      "\n",
      "  [[-0.40625   -0.4375    -0.5234375]\n",
      "   [-0.4140625 -0.4453125 -0.53125  ]\n",
      "   [-0.4140625 -0.4453125 -0.53125  ]\n",
      "   ...\n",
      "   [-0.296875  -0.328125  -0.4140625]\n",
      "   [-0.296875  -0.328125  -0.4140625]\n",
      "   [-0.3046875 -0.3359375 -0.421875 ]]\n",
      "\n",
      "  [[-0.4140625 -0.4453125 -0.53125  ]\n",
      "   [-0.4140625 -0.4453125 -0.53125  ]\n",
      "   [-0.421875  -0.453125  -0.5390625]\n",
      "   ...\n",
      "   [-0.3125    -0.34375   -0.4296875]\n",
      "   [-0.3125    -0.34375   -0.4296875]\n",
      "   [-0.3125    -0.34375   -0.4296875]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.578125  -0.6171875 -0.6640625]\n",
      "   [-0.5703125 -0.609375  -0.65625  ]\n",
      "   [-0.5625    -0.6015625 -0.6484375]\n",
      "   ...\n",
      "   [-0.5859375 -0.65625   -0.6953125]\n",
      "   [-0.578125  -0.6484375 -0.6875   ]\n",
      "   [-0.5703125 -0.640625  -0.6796875]]\n",
      "\n",
      "  [[-0.6015625 -0.640625  -0.6875   ]\n",
      "   [-0.59375   -0.6328125 -0.6796875]\n",
      "   [-0.5859375 -0.625     -0.671875 ]\n",
      "   ...\n",
      "   [-0.578125  -0.6484375 -0.6875   ]\n",
      "   [-0.5625    -0.6328125 -0.671875 ]\n",
      "   [-0.5546875 -0.625     -0.6640625]]\n",
      "\n",
      "  [[-0.625     -0.6640625 -0.7109375]\n",
      "   [-0.6171875 -0.65625   -0.703125 ]\n",
      "   [-0.6015625 -0.640625  -0.6875   ]\n",
      "   ...\n",
      "   [-0.5703125 -0.640625  -0.6796875]\n",
      "   [-0.5546875 -0.625     -0.6640625]\n",
      "   [-0.546875  -0.6171875 -0.65625  ]]]\n",
      "\n",
      "\n",
      " [[[ 0.421875   0.328125   0.359375 ]\n",
      "   [ 0.3984375  0.3125     0.34375  ]\n",
      "   [ 0.3984375  0.3125     0.34375  ]\n",
      "   ...\n",
      "   [ 0.2265625  0.25       0.3671875]\n",
      "   [ 0.2265625  0.2578125  0.3515625]\n",
      "   [ 0.2421875  0.2734375  0.3671875]]\n",
      "\n",
      "  [[ 0.4296875  0.3359375  0.3671875]\n",
      "   [ 0.3984375  0.3125     0.34375  ]\n",
      "   [ 0.390625   0.3203125  0.34375  ]\n",
      "   ...\n",
      "   [ 0.2890625  0.3203125  0.4140625]\n",
      "   [ 0.3046875  0.3359375  0.4296875]\n",
      "   [ 0.296875   0.328125   0.421875 ]]\n",
      "\n",
      "  [[ 0.4296875  0.3515625  0.359375 ]\n",
      "   [ 0.3984375  0.328125   0.3359375]\n",
      "   [ 0.375      0.328125   0.34375  ]\n",
      "   ...\n",
      "   [ 0.1640625  0.1796875  0.28125  ]\n",
      "   [ 0.1796875  0.1953125  0.296875 ]\n",
      "   [ 0.203125   0.21875    0.3125   ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.796875  -0.7890625 -0.828125 ]\n",
      "   [-0.796875  -0.7890625 -0.828125 ]\n",
      "   [-0.8046875 -0.796875  -0.8359375]\n",
      "   ...\n",
      "   [-0.578125  -0.5703125 -0.53125  ]\n",
      "   [-0.640625  -0.6328125 -0.59375  ]\n",
      "   [-0.578125  -0.5703125 -0.53125  ]]\n",
      "\n",
      "  [[-0.796875  -0.7890625 -0.8359375]\n",
      "   [-0.796875  -0.7890625 -0.8359375]\n",
      "   [-0.8046875 -0.796875  -0.84375  ]\n",
      "   ...\n",
      "   [-0.5859375 -0.578125  -0.5390625]\n",
      "   [-0.6640625 -0.65625   -0.6171875]\n",
      "   [-0.640625  -0.6328125 -0.6015625]]\n",
      "\n",
      "  [[-0.796875  -0.7890625 -0.8359375]\n",
      "   [-0.796875  -0.7890625 -0.8359375]\n",
      "   [-0.8046875 -0.796875  -0.84375  ]\n",
      "   ...\n",
      "   [-0.6171875 -0.609375  -0.5703125]\n",
      "   [-0.6640625 -0.65625   -0.625    ]\n",
      "   [-0.671875  -0.6640625 -0.6328125]]]\n",
      "\n",
      "\n",
      " [[[-0.21875   -0.4296875 -0.515625 ]\n",
      "   [-0.21875   -0.4296875 -0.515625 ]\n",
      "   [-0.21875   -0.4296875 -0.515625 ]\n",
      "   ...\n",
      "   [-0.6171875 -0.5859375 -0.5      ]\n",
      "   [-0.6171875 -0.5859375 -0.5      ]\n",
      "   [-0.6171875 -0.5859375 -0.5      ]]\n",
      "\n",
      "  [[-0.2265625 -0.4375    -0.5234375]\n",
      "   [-0.2265625 -0.4375    -0.5234375]\n",
      "   [-0.2265625 -0.4375    -0.5234375]\n",
      "   ...\n",
      "   [-0.6171875 -0.5859375 -0.5      ]\n",
      "   [-0.6171875 -0.5859375 -0.5      ]\n",
      "   [-0.6171875 -0.5859375 -0.5      ]]\n",
      "\n",
      "  [[-0.2265625 -0.4375    -0.5234375]\n",
      "   [-0.2265625 -0.4375    -0.5234375]\n",
      "   [-0.2265625 -0.4375    -0.5234375]\n",
      "   ...\n",
      "   [-0.609375  -0.578125  -0.4921875]\n",
      "   [-0.609375  -0.578125  -0.4921875]\n",
      "   [-0.609375  -0.578125  -0.4921875]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   [ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   [ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   ...\n",
      "   [-0.328125  -0.359375  -0.4296875]\n",
      "   [-0.390625  -0.421875  -0.4921875]\n",
      "   [-0.4296875 -0.4609375 -0.53125  ]]\n",
      "\n",
      "  [[ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   [ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   [ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   ...\n",
      "   [-0.375     -0.3984375 -0.46875  ]\n",
      "   [-0.4375    -0.4609375 -0.53125  ]\n",
      "   [-0.4609375 -0.484375  -0.5546875]]\n",
      "\n",
      "  [[ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   [ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   [ 0.1015625 -0.1484375 -0.265625 ]\n",
      "   ...\n",
      "   [-0.390625  -0.4140625 -0.484375 ]\n",
      "   [-0.4453125 -0.46875   -0.5390625]\n",
      "   [-0.4609375 -0.484375  -0.5546875]]]]\n",
      "X_train_preprocessed.shape: (5913, 224, 224, 3)\n",
      "y_train: [0 1 0 ... 1 1 0]\n",
      "y_train.shape: (5913,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "X_train_preprocessed = X_train / 128\n",
    "X_train_preprocessed -= 1\n",
    "print(\"X_train_preprocessed:\", X_train_preprocessed)\n",
    "print(\"X_train_preprocessed.shape:\", X_train_preprocessed.shape)\n",
    "print(\"y_train:\", y_train)\n",
    "print(\"y_train.shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['undamaged', 'damaged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_6\n",
      "1 conv2d_470\n",
      "2 batch_normalization_470\n",
      "3 activation_470\n",
      "4 conv2d_471\n",
      "5 batch_normalization_471\n",
      "6 activation_471\n",
      "7 conv2d_472\n",
      "8 batch_normalization_472\n",
      "9 activation_472\n",
      "10 max_pooling2d_20\n",
      "11 conv2d_473\n",
      "12 batch_normalization_473\n",
      "13 activation_473\n",
      "14 conv2d_474\n",
      "15 batch_normalization_474\n",
      "16 activation_474\n",
      "17 max_pooling2d_21\n",
      "18 conv2d_478\n",
      "19 batch_normalization_478\n",
      "20 activation_478\n",
      "21 conv2d_476\n",
      "22 conv2d_479\n",
      "23 batch_normalization_476\n",
      "24 batch_normalization_479\n",
      "25 activation_476\n",
      "26 activation_479\n",
      "27 average_pooling2d_45\n",
      "28 conv2d_475\n",
      "29 conv2d_477\n",
      "30 conv2d_480\n",
      "31 conv2d_481\n",
      "32 batch_normalization_475\n",
      "33 batch_normalization_477\n",
      "34 batch_normalization_480\n",
      "35 batch_normalization_481\n",
      "36 activation_475\n",
      "37 activation_477\n",
      "38 activation_480\n",
      "39 activation_481\n",
      "40 mixed0\n",
      "41 conv2d_485\n",
      "42 batch_normalization_485\n",
      "43 activation_485\n",
      "44 conv2d_483\n",
      "45 conv2d_486\n",
      "46 batch_normalization_483\n",
      "47 batch_normalization_486\n",
      "48 activation_483\n",
      "49 activation_486\n",
      "50 average_pooling2d_46\n",
      "51 conv2d_482\n",
      "52 conv2d_484\n",
      "53 conv2d_487\n",
      "54 conv2d_488\n",
      "55 batch_normalization_482\n",
      "56 batch_normalization_484\n",
      "57 batch_normalization_487\n",
      "58 batch_normalization_488\n",
      "59 activation_482\n",
      "60 activation_484\n",
      "61 activation_487\n",
      "62 activation_488\n",
      "63 mixed1\n",
      "64 conv2d_492\n",
      "65 batch_normalization_492\n",
      "66 activation_492\n",
      "67 conv2d_490\n",
      "68 conv2d_493\n",
      "69 batch_normalization_490\n",
      "70 batch_normalization_493\n",
      "71 activation_490\n",
      "72 activation_493\n",
      "73 average_pooling2d_47\n",
      "74 conv2d_489\n",
      "75 conv2d_491\n",
      "76 conv2d_494\n",
      "77 conv2d_495\n",
      "78 batch_normalization_489\n",
      "79 batch_normalization_491\n",
      "80 batch_normalization_494\n",
      "81 batch_normalization_495\n",
      "82 activation_489\n",
      "83 activation_491\n",
      "84 activation_494\n",
      "85 activation_495\n",
      "86 mixed2\n",
      "87 conv2d_497\n",
      "88 batch_normalization_497\n",
      "89 activation_497\n",
      "90 conv2d_498\n",
      "91 batch_normalization_498\n",
      "92 activation_498\n",
      "93 conv2d_496\n",
      "94 conv2d_499\n",
      "95 batch_normalization_496\n",
      "96 batch_normalization_499\n",
      "97 activation_496\n",
      "98 activation_499\n",
      "99 max_pooling2d_22\n",
      "100 mixed3\n",
      "101 conv2d_504\n",
      "102 batch_normalization_504\n",
      "103 activation_504\n",
      "104 conv2d_505\n",
      "105 batch_normalization_505\n",
      "106 activation_505\n",
      "107 conv2d_501\n",
      "108 conv2d_506\n",
      "109 batch_normalization_501\n",
      "110 batch_normalization_506\n",
      "111 activation_501\n",
      "112 activation_506\n",
      "113 conv2d_502\n",
      "114 conv2d_507\n",
      "115 batch_normalization_502\n",
      "116 batch_normalization_507\n",
      "117 activation_502\n",
      "118 activation_507\n",
      "119 average_pooling2d_48\n",
      "120 conv2d_500\n",
      "121 conv2d_503\n",
      "122 conv2d_508\n",
      "123 conv2d_509\n",
      "124 batch_normalization_500\n",
      "125 batch_normalization_503\n",
      "126 batch_normalization_508\n",
      "127 batch_normalization_509\n",
      "128 activation_500\n",
      "129 activation_503\n",
      "130 activation_508\n",
      "131 activation_509\n",
      "132 mixed4\n",
      "133 conv2d_514\n",
      "134 batch_normalization_514\n",
      "135 activation_514\n",
      "136 conv2d_515\n",
      "137 batch_normalization_515\n",
      "138 activation_515\n",
      "139 conv2d_511\n",
      "140 conv2d_516\n",
      "141 batch_normalization_511\n",
      "142 batch_normalization_516\n",
      "143 activation_511\n",
      "144 activation_516\n",
      "145 conv2d_512\n",
      "146 conv2d_517\n",
      "147 batch_normalization_512\n",
      "148 batch_normalization_517\n",
      "149 activation_512\n",
      "150 activation_517\n",
      "151 average_pooling2d_49\n",
      "152 conv2d_510\n",
      "153 conv2d_513\n",
      "154 conv2d_518\n",
      "155 conv2d_519\n",
      "156 batch_normalization_510\n",
      "157 batch_normalization_513\n",
      "158 batch_normalization_518\n",
      "159 batch_normalization_519\n",
      "160 activation_510\n",
      "161 activation_513\n",
      "162 activation_518\n",
      "163 activation_519\n",
      "164 mixed5\n",
      "165 conv2d_524\n",
      "166 batch_normalization_524\n",
      "167 activation_524\n",
      "168 conv2d_525\n",
      "169 batch_normalization_525\n",
      "170 activation_525\n",
      "171 conv2d_521\n",
      "172 conv2d_526\n",
      "173 batch_normalization_521\n",
      "174 batch_normalization_526\n",
      "175 activation_521\n",
      "176 activation_526\n",
      "177 conv2d_522\n",
      "178 conv2d_527\n",
      "179 batch_normalization_522\n",
      "180 batch_normalization_527\n",
      "181 activation_522\n",
      "182 activation_527\n",
      "183 average_pooling2d_50\n",
      "184 conv2d_520\n",
      "185 conv2d_523\n",
      "186 conv2d_528\n",
      "187 conv2d_529\n",
      "188 batch_normalization_520\n",
      "189 batch_normalization_523\n",
      "190 batch_normalization_528\n",
      "191 batch_normalization_529\n",
      "192 activation_520\n",
      "193 activation_523\n",
      "194 activation_528\n",
      "195 activation_529\n",
      "196 mixed6\n",
      "197 conv2d_534\n",
      "198 batch_normalization_534\n",
      "199 activation_534\n",
      "200 conv2d_535\n",
      "201 batch_normalization_535\n",
      "202 activation_535\n",
      "203 conv2d_531\n",
      "204 conv2d_536\n",
      "205 batch_normalization_531\n",
      "206 batch_normalization_536\n",
      "207 activation_531\n",
      "208 activation_536\n",
      "209 conv2d_532\n",
      "210 conv2d_537\n",
      "211 batch_normalization_532\n",
      "212 batch_normalization_537\n",
      "213 activation_532\n",
      "214 activation_537\n",
      "215 average_pooling2d_51\n",
      "216 conv2d_530\n",
      "217 conv2d_533\n",
      "218 conv2d_538\n",
      "219 conv2d_539\n",
      "220 batch_normalization_530\n",
      "221 batch_normalization_533\n",
      "222 batch_normalization_538\n",
      "223 batch_normalization_539\n",
      "224 activation_530\n",
      "225 activation_533\n",
      "226 activation_538\n",
      "227 activation_539\n",
      "228 mixed7\n",
      "229 conv2d_542\n",
      "230 batch_normalization_542\n",
      "231 activation_542\n",
      "232 conv2d_543\n",
      "233 batch_normalization_543\n",
      "234 activation_543\n",
      "235 conv2d_540\n",
      "236 conv2d_544\n",
      "237 batch_normalization_540\n",
      "238 batch_normalization_544\n",
      "239 activation_540\n",
      "240 activation_544\n",
      "241 conv2d_541\n",
      "242 conv2d_545\n",
      "243 batch_normalization_541\n",
      "244 batch_normalization_545\n",
      "245 activation_541\n",
      "246 activation_545\n",
      "247 max_pooling2d_23\n",
      "248 mixed8\n",
      "249 conv2d_550\n",
      "250 batch_normalization_550\n",
      "251 activation_550\n",
      "252 conv2d_547\n",
      "253 conv2d_551\n",
      "254 batch_normalization_547\n",
      "255 batch_normalization_551\n",
      "256 activation_547\n",
      "257 activation_551\n",
      "258 conv2d_548\n",
      "259 conv2d_549\n",
      "260 conv2d_552\n",
      "261 conv2d_553\n",
      "262 average_pooling2d_52\n",
      "263 conv2d_546\n",
      "264 batch_normalization_548\n",
      "265 batch_normalization_549\n",
      "266 batch_normalization_552\n",
      "267 batch_normalization_553\n",
      "268 conv2d_554\n",
      "269 batch_normalization_546\n",
      "270 activation_548\n",
      "271 activation_549\n",
      "272 activation_552\n",
      "273 activation_553\n",
      "274 batch_normalization_554\n",
      "275 activation_546\n",
      "276 mixed9_0\n",
      "277 concatenate_10\n",
      "278 activation_554\n",
      "279 mixed9\n",
      "280 conv2d_559\n",
      "281 batch_normalization_559\n",
      "282 activation_559\n",
      "283 conv2d_556\n",
      "284 conv2d_560\n",
      "285 batch_normalization_556\n",
      "286 batch_normalization_560\n",
      "287 activation_556\n",
      "288 activation_560\n",
      "289 conv2d_557\n",
      "290 conv2d_558\n",
      "291 conv2d_561\n",
      "292 conv2d_562\n",
      "293 average_pooling2d_53\n",
      "294 conv2d_555\n",
      "295 batch_normalization_557\n",
      "296 batch_normalization_558\n",
      "297 batch_normalization_561\n",
      "298 batch_normalization_562\n",
      "299 conv2d_563\n",
      "300 batch_normalization_555\n",
      "301 activation_557\n",
      "302 activation_558\n",
      "303 activation_561\n",
      "304 activation_562\n",
      "305 batch_normalization_563\n",
      "306 activation_555\n",
      "307 mixed9_1\n",
      "308 concatenate_11\n",
      "309 activation_563\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "inception_v3 = tf.keras.applications.InceptionV3(input_shape=(224, 224, 3), include_top=False)\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(inception_v3.layers):\n",
    "    print(i, layer.name)\n",
    "# we will freeze the first 249 layers and unfreeze the rest:\n",
    "for layer in inception_v3.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in inception_v3.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  inception_v3,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(len(label_names), activation=\"softmax\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), \n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 21,806,882\n",
      "Trainable params: 11,118,978\n",
      "Non-trainable params: 10,687,904\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/54 [====>.........................] - ETA: 14:11 - loss: 1.0221 - acc: 0.6520"
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train_preprocessed, y=y_train, batch_size=100, epochs=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
